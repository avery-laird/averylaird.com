---
layout: post
title: "Superintelligence and the Nature of Consciouness"
categories: programming future superintelligence machine-learning
---

Notable leaders in the technology and science industries (Elon Musk, Bill Gates, and Stephen Hawking to name a few) have  proposed that our greatest collective fear --- moving into the age of smart machines --- ought to be the manifestation of superintelligence. This term is popularized (or coined, it's a bit unclear) by the 2014 book of the same name, by Nick Bostrom. However, the fear of being powerless in the face of a superior artificial intelligence is not new.

Although a thorough and cautious consideration of the implications of any new technology is essential, I think much of this fear is unwarranted. Here's why.

There are a few ways we can imagine superintelligence (that is, an artificial intelligence with superhuman capabilities) coming to be. It would *probably* be created by a large organization, nation-state, or even an individual, using the following techniques:

* machine learning
* brain simulation

Much of the fear lies in the first method, because we could be dealing with something wholly inhuman, which may not share our common values of morality or other rules of the current society. Although that's not even particularly true of the human race today, the idea is that a machine may not "care:" its actions may not be desired, predictable, or understandable by any human.

The main problem with this theory is that it is completely unclear whether we will ever be able to create an intelligence, in the sense of a sentient being. We currently know hardly anything about the nature of consciousness within ourselves, and what makes us sentient; what makes us "different" than all other animals we have encountered. Given this, the manifestation of superintelligence through machine learning presupposes that a sentient being will slowly emerge from an algorithmic fog, without us having to understand how it happened.

Machine learning is indeed impressive, and a little scary at times, but it is not magic (or perhaps, just not a sufficiently advanced technology). It works by modeling something us humans can do very well --- learn. However, it does *not* model how a human learns, rather it "learns" in a abstract way --- making better and better guesses. There is a lot more to being a human than learning, and largely for this reason, it seems unlikely that machine learning will "organically" yield a superintelligence. Its easy to think of how much a human baby learns in its early years, and consider it plausible that machine learning might somehow give a similar result. Yet there is so much we don't know --- for example, an ape can learn as well. They can learn sign language, and communicate in a rudimentary way, but they never ask questions. On the other hand, a parrot named Alex once asked what colour he was.

These kind of experiments raise more questions than answers: some say that the apes never *actually* learned to communicate, but rather performed a clever trick for a reward. Of course, this is almost impossible to prove or disprove, even for humans --- what does it mean to communicate? Do we actually communicate, or simply perform elaborate actions defined by some evolutionary rule? Our completely inability to address such questions shows just how little we *truly* know about sentience.

It seems reasonable that there may be a spectrum of intelligence, rather than a clear border, between the sentience of a human and the behaviour observed in other animals. This spectrum may extend far past human intelligence, and this is where superintelligence comes into play.

If we consider a superintelligence reached by the second method, brain simulation, we might propose that we may raise the intelligence of that brain above the human range, perhaps by "speeding up the clock" (making the simulation run faster), providing more memory, making some modifications to existing structures, and so on. Speeding up the clock would likely allow the brain to operate much faster than its perception of the outside world (and likewise, our perception of the brain's time), but who knows what effect this would have on intelligence. 


It would seem this is mostly due to a failure to separate Artificial Intelligence from Superintelligence. These are related, but different in subtle yet important ways.
